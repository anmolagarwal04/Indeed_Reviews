{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import textstat\n",
    "from textblob import TextBlob\n",
    "\n",
    "# languange processing imports\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from gensim.corpora import Dictionary\n",
    "# preprocessing imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# model imports\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# hyperparameter training imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# visualization imports\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import base64\n",
    "import io\n",
    "%matplotlib inline\n",
    "sns.set()  # defines the style of 'the plots to be seaborn style\n",
    "words = set(nltk.corpus.words.words())\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.drop(columns=['Symbol'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('Final/reviews_analysed_combined.csv',lineterminator='\\n')\n",
    "reviews =reviews.drop(columns='Unnamed: 0',axis=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.Company.value_counts().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liwc = pd.read_csv('LIWC.csv',header=0,index_col=0)\n",
    "reviews = pd.read_csv('Reviews/final/reviews_combined.csv')\n",
    "reviews = reviews.drop_duplicates(subset = 'Text',keep='first')\n",
    "reviews = reviews[reviews['Text'].notna()]\n",
    "reviews = reviews.drop(columns=['Symbol'],axis=1)\n",
    "reviews = reviews[reviews['Text']!='Review']\n",
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('Final/Reviews_en.csv',index_col=0,lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_length = []\n",
    "word_length = []\n",
    "for review in reviews.text_processed:\n",
    "    words = review.split()\n",
    "    word_length.append(len(words))\n",
    "    char_length.append(len(review))\n",
    "reviews['No_of_Words'] = pd.Series(word_length)\n",
    "reviews['No_of_Chars'] = pd.Series(char_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Emotiveness'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langdetect import detect\n",
    "# from tqdm import tqdm_notebook\n",
    "# tqdm_notebook().pandas()\n",
    "# reviews['lang'] = reviews.Text.progress_map(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews= reviews[reviews['lang']=='en']\n",
    "reviews.to_csv('Final/Reviews_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in reviews.columns:\n",
    "    print(column)\n",
    "    print(reviews[column].isnull().sum())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langdetect import detect       \n",
    "def detectlang(review):\n",
    "    try:\n",
    "        return detect(review)\n",
    "    except:\n",
    "        return ' '\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = []\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    for i,analyse in enumerate(executor.map(detectlang,reviews.Text)):\n",
    "        if(i%10000==0):\n",
    "            print(i)\n",
    "        language.append(analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['lang'] = pd.Series(language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard = pd.read_csv('Harvard_Modified.csv')\n",
    "liwc = pd.read_csv('LIWC.csv')\n",
    "# reviews['lang'] = reviews.Text.progress_map(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_harvard = ['Active','Passive','Need','Goal','Try','Means','Persist','Complet','Finish','PowGain','PowLoss','PowCon','PowCoop'\n",
    "             ,'PowAuth','PowTot','NegAff','PosAff','SureLw','Self','Our','You']\n",
    "\n",
    "# harvard_modified = pd.DataFrame()\n",
    "categories_dict_harvard = {}\n",
    "# for category in categories_harvard:\n",
    "#     harvard_modified[category] = pd.Series(list(harvard[category].dropna()))\n",
    "for category in categories_harvard:\n",
    "    edited = []\n",
    "    for word in harvard[category]:\n",
    "        word = str(word).split('#')[0]\n",
    "        word = ' ' + word + ' '\n",
    "        edited.append(word)\n",
    "    edited_series = pd.Series(edited)\n",
    "    edited_series = pd.Series(edited_series.unique())\n",
    "    harvard[category] = edited_series.dropna()\n",
    "#     harvard_modified[category] = harvard_modified[category].unique()\n",
    "\n",
    "for category in categories_harvard:\n",
    "    values = pd.Series(list(harvard[category].dropna()))\n",
    "    categories_dict_harvard[category] = values\n",
    "    \n",
    "our_self_you = list(categories_dict_harvard['Our']) + list(categories_dict_harvard['Self']) +list(categories_dict_harvard['You'])\n",
    "our_self = list(categories_dict_harvard['Our']) + list(categories_dict_harvard['Self'])\n",
    "categories_dict_harvard['Our_Self_You'] = pd.Series(our_self_you).unique()\n",
    "categories_dict_harvard['Our_Self'] = pd.Series(our_self).unique()\n",
    "\n",
    "categories_harvard_avg = []\n",
    "for category in categories_harvard:\n",
    "    categories_harvard_avg.append(category+'_avg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_liwc = ['Money','Work','Swear','Anger','Anx','Sad','Tentat','Certain','FocusPast','FocusPresent',\n",
    "              'FocusFuture','Posemo','Negemo','Percept','See','Hear','Feel']\n",
    "\n",
    "categories_dict_liwc = {}\n",
    "for category in categories_liwc:\n",
    "    edited = []\n",
    "    for word in liwc[category]:\n",
    "#         word = str(word).split('#')[0]\n",
    "#         word = ' ' + str(word) + ' '\n",
    "        edited.append(word)\n",
    "    edited_series = pd.Series(edited)\n",
    "    edited_series = pd.Series(edited_series.unique())\n",
    "    liwc[category] = edited_series.dropna()\n",
    "#     harvard_modified[category] = harvard_modified[category].unique()\n",
    "\n",
    "for category in categories_liwc:\n",
    "    values = pd.Series(list(liwc[category].dropna()))\n",
    "    categories_dict_liwc[category] = values\n",
    "    \n",
    "categories_dict_liwc['pisr'] = pd.Series(list(categories_dict_liwc['Percept'])+list(categories_dict_liwc['Hear']) +\n",
    "                                         list(categories_dict_liwc['See'])+ \n",
    "                                         list(categories_dict_liwc['Feel'])).unique()\n",
    "\n",
    "categories_liwc_avg = []\n",
    "for category in categories_liwc:\n",
    "    categories_liwc_avg.append(category+'_avg')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.concat([reviews,pd.DataFrame(columns=categories_liwc)])\n",
    "reviews = pd.concat([reviews,pd.DataFrame(columns=categories_harvard)])\n",
    "reviews = pd.concat([reviews,pd.DataFrame(columns=categories_liwc_avg)])\n",
    "reviews = pd.concat([reviews,pd.DataFrame(columns=categories_harvard_avg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #employment status\n",
    "# employ = []\n",
    "# for review in reviews.Designation:\n",
    "#     status = str(review).split('(')\n",
    "#     if(len(status)>1):\n",
    "#         employ.append(status[1].split(')')[0])\n",
    "#     else:\n",
    "#         employ.append('')\n",
    "# reviews['Employment_Status'] = pd.Series(employ) \n",
    "def status(desig):\n",
    "    if 'Former' in str(desig):\n",
    "        return 0\n",
    "    elif 'Current' in str(desig):\n",
    "        return 1\n",
    "    else:\n",
    "        return float('NaN')\n",
    "    \n",
    "status1 = []\n",
    "import concurrent.futures\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    for i,analyse in enumerate(executor.map(status,reviews['Designation'])):\n",
    "#         print(i)\n",
    "        status1.append(analyse)\n",
    "reviews['Employment_Status'] = pd.Series(status1)\n",
    "#         reviews[i,'Employment_status'] = analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "reviews['text_processed'] = reviews['Text'].map(lambda x: re.sub('[\\(\\),\\.!?]+', '', x))\n",
    "# Convert the titles to lowercase\n",
    "reviews['text_processed'] = reviews['Text'].map(lambda x: x.lower())\n",
    "reviews['text_processed'] = reviews['Text'].map(lambda x: ' ' + x + ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_tokens(sentence):\n",
    "    replaced_punctation = list(map(lambda token: re.sub('[^0-9A-Za-z!?]+', '', token), sentence))\n",
    "    removed_punctation = list(filter(lambda token: token, replaced_punctation))\n",
    "    return removed_punctation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "executor = concurrent.futures.ProcessPoolExecutor() \n",
    "def lda_get_good_tokens(df):\n",
    "    df['Text'] = df.Text.str.lower()\n",
    "    df['tokenized_text'] = list(map(nltk.word_tokenize, df.Text))\n",
    "    df['tokenized_text'] = list(map(get_good_tokens, df.tokenized_text))\n",
    "#     df['tags'] = list(map(nltk.pos_tag,df.tokenized_text))\n",
    "\n",
    "lda_get_good_tokens(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = concurrent.futures.ProcessPoolExecutor() \n",
    "reviews['tags'] = list(executor.map(nltk.pos_tag,reviews.tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(reviews.tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv('Final/Reviews_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "count = []\n",
    "for words in reviews.tokenized_text:\n",
    "    errors = len(spell.unknown(words))\n",
    "    count.append(errors)\n",
    "reviews['Spell_Errors'] = pd.Series(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion(review):\n",
    "    nouns = len([item[0] for item in review if item[1][0] == 'N'])\n",
    "    adv = len([item[0] for item in review if item[1][0] == 'R'])\n",
    "    adj = len([item[0] for item in review if item[1][0] == 'J'])\n",
    "    verb = len([item[0] for item in review if item[1][0] == 'V'])\n",
    "    try:\n",
    "        return (adj+adv)/(nouns+verb)\n",
    "    except:\n",
    "        \n",
    "        return -1\n",
    "#     emotiveness.append(emotive)\n",
    "# reviews[\"Emotiveness\"] = pd.Series(emotiveness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotiveness = []\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    for i,analyse in enumerate(executor.map(emotion,reviews.tags)):\n",
    "        print(i)\n",
    "#         analyse+=1\n",
    "#         print(analyse[0])\n",
    "        emotiveness.append(analyse)\n",
    "#     except:\n",
    "#         emotiveness.append('-1')\n",
    "    \n",
    "reviews[\"Emotiveness\"] = pd.Series(emotiveness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = reviews.tags[0]\n",
    "nouns = len([item[0] for item in temp if item[1][0] == 'N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['PISR'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pisr\n",
    "\n",
    "def pisrs(review):\n",
    "    count =0\n",
    "    words = review.split()\n",
    "    word_length = len(words)\n",
    "    for term in categories_dict_liwc['pisr']:\n",
    "        count+=len(re.findall(term,review))\n",
    "    return count/word_length\n",
    "#     pisr.append(count/len(review))\n",
    "# reviews['PISR'] = pd.Series(pisr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pisr = []\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    for i,analyse in enumerate(executor.map(pisrs,reviews.text_processed)):\n",
    "#         print(i)\n",
    "        pisr.append(analyse)\n",
    "reviews['PISR'] = pd.Series(pisr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores =[]\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    for i,analyse in enumerate(executor.map(textstat.dale_chall_readability_score,reviews.text_processed)):\n",
    "        scores.append(analyse)\n",
    "reviews['Readability Score'] = pd.Series(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectivity = []\n",
    "polarity =[]\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    for i,analysis in enumerate(executor.map(TextBlob,reviews.text_processed)):\n",
    "#         print(i)\n",
    "        subjectivity.append(analysis.sentiment.subjectivity)\n",
    "        polarity.append(analysis.sentiment.polarity)\n",
    "                \n",
    "reviews['Subjectivity'] = pd.Series(subjectivity)\n",
    "reviews['Polarity'] = pd.Series(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def harvard_analysis(review):\n",
    "#     results =[]\n",
    "#     results_avg = []\n",
    "#     for category in categories_harvard:\n",
    "# #         print(category)\n",
    "# #         print()\n",
    "#         count = 0\n",
    "#         for term in categories_dict_harvard[category]:\n",
    "# #             print(term)\n",
    "#             words = re.findall(term,review)\n",
    "#             count += len(words)\n",
    "#         avg = count/len(review)\n",
    "# #         reviews.loc[i,'length'] = len(review)\n",
    "#         results.append(count)\n",
    "#         results_avg.append(avg)\n",
    "# #         opinion.loc[i,category] = count\n",
    "# #         category_avg = str(category) + '_avg'\n",
    "# #         opinion.loc[i,category_avg] = avg\n",
    "#     return [results,results_avg];\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "# results_avg = []\n",
    "# with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#     for i,analysis in enumerate(executor.map(harvard_analysis,reviews.text_processed)):\n",
    "#         print(i)\n",
    "#         reviews.loc[i,categories_harvard]= analysis[0]\n",
    "#         reviews.loc[i,categories_harvard_avg] = analysis[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvard_analysis(review,category):\n",
    "#     print(categories_harvard[i])\n",
    "    count = 0  \n",
    "    words = review.split()\n",
    "    word_length = len(words)\n",
    "    for term in categories_dict_harvard[category]:\n",
    "        words = re.findall(term,review)\n",
    "        count += len(words)\n",
    "    avg = count/word_length\n",
    "    return [count,avg];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools\n",
    "executor = concurrent.futures.ProcessPoolExecutor()\n",
    "for i in range(len(categories_harvard)):\n",
    "    categori = categories_harvard[i]\n",
    "    print(categories_harvard[i])\n",
    "    series1 = []\n",
    "    series2 = []\n",
    "    for j,analyse in enumerate(executor.map(harvard_analysis,reviews.text_processed,itertools.repeat(categori))):\n",
    "        if(j%10000==0):\n",
    "            print(j)\n",
    "        series1.append(analyse[0])\n",
    "        series2.append(analyse[1])\n",
    "    reviews[str(categories_harvard[i])] = series1\n",
    "    reviews[str(categories_harvard[i]) +'_avg'] = series2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liwc_analysis(review,category):\n",
    "#     print(category)\n",
    "    count = 0  \n",
    "    words = review.split()\n",
    "    word_length = len(words)\n",
    "#     for term in categories_dict_liwc[categories_liwc[i]]:\n",
    "    for term in categories_dict_liwc[category]:\n",
    "        words = re.findall(term,review)\n",
    "        count += len(words)\n",
    "#         print(term)\n",
    "    avg = count/word_length\n",
    "    return [count,avg];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools\n",
    "executor = concurrent.futures.ProcessPoolExecutor()\n",
    "for i in range(len(categories_liwc)):\n",
    "    categori = categories_liwc[i]\n",
    "#     print(i)\n",
    "    print(categori)\n",
    "    series1 = []\n",
    "    series2 = []\n",
    "    for j,analyse in enumerate(executor.map(liwc_analysis,reviews.text_processed,itertools.repeat(categori))):\n",
    "        if(j%100000==0):\n",
    "            print(j)\n",
    "        series1.append(analyse[0])\n",
    "        series2.append(analyse[1])\n",
    "    reviews[str(categories_liwc[i])] = series1\n",
    "    reviews[str(categories_liwc[i]) +'_avg'] = series2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv('Final/reviews_analysed_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv('reviews_analysed1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_ref(review):\n",
    "    count_our = 0\n",
    "    count_self = 0\n",
    "    count_our_self = 0\n",
    "    count_our_self_you = 0\n",
    "    words = review.split()\n",
    "    word_length = len(words)\n",
    "#     print(i)\n",
    "    for term in categories_dict_harvard['Our']:\n",
    "        count_our+= len(re.findall(term,review))\n",
    "\n",
    "    for term in categories_dict_harvard['Self']:\n",
    "        count_self += len(re.findall(term,review))\n",
    "\n",
    "    for term in categories_dict_harvard['Our_Self']:\n",
    "        count_our_self+= len(re.findall(term,review))\n",
    "    for term in categories_dict_harvard['Our_Self_You']:\n",
    "        count_our_self_you+= len(re.findall(term,review))\n",
    "\n",
    "    if(count_our_self == 0):\n",
    "        return [count_our/word_length,count_our_self_you/word_length,count_self/word_length,-1];\n",
    "    else:\n",
    "        return [count_our/word_length,count_our_self_you/word_length,count_self/word_length,count_self/count_our_self];\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_reference = []\n",
    "group_reference = []\n",
    "narcissicism = []\n",
    "self_ref = []\n",
    "\n",
    "import concurrent.futures\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    for i,analyse in enumerate(executor.map(individual_ref,reviews.text_processed)):\n",
    "        if(i%100000==0):\n",
    "            print(i)\n",
    "#         analyse+=1\n",
    "#         print(analyse[0])\n",
    "        individual_reference.append(analyse)\n",
    "#         group_reference.append(analyse[1])\n",
    "#         self_ref.append(analyse[2])\n",
    "#         narcissicism.append(analyse[3])\n",
    "        \n",
    "        \n",
    "        \n",
    "reviews['Individual_Reference'] = pd.Series(individual_reference)\n",
    "# reviews['Group_Reference'] = pd.Series(group_reference)\n",
    "# reviews['Self_Reference'] = pd.Series(self_ref)\n",
    "# reviews['Narcissism_Reference'] = pd.Series(narcissicism)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_ref(review):\n",
    "    count_our = 0\n",
    "    count_self = 0\n",
    "    count_our_self = 0\n",
    "    count_our_self_you = 0\n",
    "    words = review.split()\n",
    "    word_length = len(words)\n",
    "#     print(i)\n",
    "    for term in categories_dict_harvard['Our']:\n",
    "        count_our+= len(re.findall(term,review))\n",
    "    count_ind = count_group\n",
    "    count_narc = count_group\n",
    "    for term in categories_dict_harvard['Self']:\n",
    "        count_self += len(re.findall(term,review))\n",
    "#     return count_our_self_you/word_length\n",
    "    count_ind+= count_self\n",
    "    count_narc+= count_self\n",
    "    for term in categories_dict_harvard['Our_Self']:\n",
    "        count_our_self+= len(re.findall(term,review))\n",
    "    for term in categories_dict_harvard['Our_Self_You']:\n",
    "        count_our_self_you+= len(re.findall(term,review))\n",
    "    if(count_our_self == 0):\n",
    "        return [count_our_self_you/word_length,count_self/word_length,-1];\n",
    "    else:\n",
    "        return [count_our_self_you/word_length,count_self/word_length,count_self/count_our_self];\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_reference = []\n",
    "group_reference = []\n",
    "narcissicism = []\n",
    "self_ref = []\n",
    "\n",
    "import concurrent.futures\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    for i,analyse in enumerate(executor.map(individual_ref,reviews.text_processed)):\n",
    "        if(i%100000==0):\n",
    "            print(i)\n",
    "#         analyse+=1\n",
    "#         print(analyse[0])\n",
    "        individual_reference.append(analyse[0])\n",
    "        group_reference.append(analyse[1])\n",
    "        self_ref.append(analyse[2])\n",
    "        narcissicism.append(analyse[3])\n",
    "        \n",
    "        \n",
    "        \n",
    "reviews['Individual_Reference'] = pd.Series(individual_reference)\n",
    "reviews['Group_Reference'] = pd.Series(group_reference)\n",
    "reviews['Self_Reference'] = pd.Series(self_ref)\n",
    "reviews['Narcissism_Reference'] = pd.Series(narcissicism)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Group_Reference'] = pd.Series(group_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv('Final/reviews_analysed_narcissism.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.drop(['lang',],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[categories_harvard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
